{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dbbco6NRPkBe"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0ifPNzmPnE9",
    "outputId": "8c823dc2-8232-4b58-990f-8eb798d2e239"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dbvE_xFcPnE-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#EEG = pd.read_csv('/content/drive/MyDrive/EEG_data_1.csv')\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m EEG \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEEG_data-1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#EEG = pd.read_csv('/content/drive/MyDrive/EEG_data_1.csv')\n",
    "EEG = pd.read_csv('EEG_data-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOxsgz6yWoEo",
    "outputId": "af150054-cac7-40f6-d563-c8792fbb9035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68831 entries, 0 to 68830\n",
      "Data columns (total 87 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   subject_id          68831 non-null  int64  \n",
      " 1   video_id            68831 non-null  int64  \n",
      " 2   EEG.AF3             68831 non-null  float64\n",
      " 3   EEG.F7              68831 non-null  float64\n",
      " 4   EEG.F3              68831 non-null  float64\n",
      " 5   EEG.FC5             68831 non-null  float64\n",
      " 6   EEG.T7              68831 non-null  float64\n",
      " 7   EEG.P7              68831 non-null  float64\n",
      " 8   EEG.O1              68831 non-null  float64\n",
      " 9   EEG.O2              68831 non-null  float64\n",
      " 10  EEG.P8              68831 non-null  float64\n",
      " 11  EEG.T8              68831 non-null  float64\n",
      " 12  EEG.FC6             68831 non-null  float64\n",
      " 13  EEG.F4              68831 non-null  float64\n",
      " 14  EEG.F8              68831 non-null  float64\n",
      " 15  EEG.AF4             68831 non-null  float64\n",
      " 16  POW.AF3.Theta       68831 non-null  float64\n",
      " 17  POW.AF3.Alpha       68831 non-null  float64\n",
      " 18  POW.AF3.BetaL       68831 non-null  float64\n",
      " 19  POW.AF3.BetaH       68831 non-null  float64\n",
      " 20  POW.AF3.Gamma       68831 non-null  float64\n",
      " 21  POW.F7.Theta        68831 non-null  float64\n",
      " 22  POW.F7.Alpha        68831 non-null  float64\n",
      " 23  POW.F7.BetaL        68831 non-null  float64\n",
      " 24  POW.F7.BetaH        68831 non-null  float64\n",
      " 25  POW.F7.Gamma        68831 non-null  float64\n",
      " 26  POW.F3.Theta        68831 non-null  float64\n",
      " 27  POW.F3.Alpha        68831 non-null  float64\n",
      " 28  POW.F3.BetaL        68831 non-null  float64\n",
      " 29  POW.F3.BetaH        68831 non-null  float64\n",
      " 30  POW.F3.Gamma        68831 non-null  float64\n",
      " 31  POW.FC5.Theta       68831 non-null  float64\n",
      " 32  POW.FC5.Alpha       68831 non-null  float64\n",
      " 33  POW.FC5.BetaL       68831 non-null  float64\n",
      " 34  POW.FC5.BetaH       68831 non-null  float64\n",
      " 35  POW.FC5.Gamma       68831 non-null  float64\n",
      " 36  POW.T7.Theta        68831 non-null  float64\n",
      " 37  POW.T7.Alpha        68831 non-null  float64\n",
      " 38  POW.T7.BetaL        68831 non-null  float64\n",
      " 39  POW.T7.BetaH        68831 non-null  float64\n",
      " 40  POW.T7.Gamma        68831 non-null  float64\n",
      " 41  POW.P7.Theta        68831 non-null  float64\n",
      " 42  POW.P7.Alpha        68831 non-null  float64\n",
      " 43  POW.P7.BetaL        68831 non-null  float64\n",
      " 44  POW.P7.BetaH        68831 non-null  float64\n",
      " 45  POW.P7.Gamma        68831 non-null  float64\n",
      " 46  POW.O1.Theta        68831 non-null  float64\n",
      " 47  POW.O1.Alpha        68831 non-null  float64\n",
      " 48  POW.O1.BetaL        68831 non-null  float64\n",
      " 49  POW.O1.BetaH        68831 non-null  float64\n",
      " 50  POW.O1.Gamma        68831 non-null  float64\n",
      " 51  POW.O2.Theta        68831 non-null  float64\n",
      " 52  POW.O2.Alpha        68831 non-null  float64\n",
      " 53  POW.O2.BetaL        68831 non-null  float64\n",
      " 54  POW.O2.BetaH        68831 non-null  float64\n",
      " 55  POW.O2.Gamma        68831 non-null  float64\n",
      " 56  POW.P8.Theta        68831 non-null  float64\n",
      " 57  POW.P8.Alpha        68831 non-null  float64\n",
      " 58  POW.P8.BetaL        68831 non-null  float64\n",
      " 59  POW.P8.BetaH        68831 non-null  float64\n",
      " 60  POW.P8.Gamma        68831 non-null  float64\n",
      " 61  POW.T8.Theta        68831 non-null  float64\n",
      " 62  POW.T8.Alpha        68831 non-null  float64\n",
      " 63  POW.T8.BetaL        68831 non-null  float64\n",
      " 64  POW.T8.BetaH        68831 non-null  float64\n",
      " 65  POW.T8.Gamma        68831 non-null  float64\n",
      " 66  POW.FC6.Theta       68831 non-null  float64\n",
      " 67  POW.FC6.Alpha       68831 non-null  float64\n",
      " 68  POW.FC6.BetaL       68831 non-null  float64\n",
      " 69  POW.FC6.BetaH       68831 non-null  float64\n",
      " 70  POW.FC6.Gamma       68831 non-null  float64\n",
      " 71  POW.F4.Theta        68831 non-null  float64\n",
      " 72  POW.F4.Alpha        68831 non-null  float64\n",
      " 73  POW.F4.BetaL        68831 non-null  float64\n",
      " 74  POW.F4.BetaH        68831 non-null  float64\n",
      " 75  POW.F4.Gamma        68831 non-null  float64\n",
      " 76  POW.F8.Theta        68831 non-null  float64\n",
      " 77  POW.F8.Alpha        68831 non-null  float64\n",
      " 78  POW.F8.BetaL        68831 non-null  float64\n",
      " 79  POW.F8.BetaH        68831 non-null  float64\n",
      " 80  POW.F8.Gamma        68831 non-null  float64\n",
      " 81  POW.AF4.Theta       68831 non-null  float64\n",
      " 82  POW.AF4.Alpha       68831 non-null  float64\n",
      " 83  POW.AF4.BetaL       68831 non-null  float64\n",
      " 84  POW.AF4.BetaH       68831 non-null  float64\n",
      " 85  POW.AF4.Gamma       68831 non-null  float64\n",
      " 86  subject_understood  68831 non-null  int64  \n",
      "dtypes: float64(84), int64(3)\n",
      "memory usage: 45.7 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   subject_id  video_id      EEG.AF3       EEG.F7       EEG.F3      EEG.FC5  \\\n",
       " 0           0         0  4210.641113  4179.102539  4287.948730  4235.384766   \n",
       " 1           0         0  4201.025879  4188.717773  4280.128418  4236.922852   \n",
       " 2           0         0  4203.205078  4182.820313  4282.820313  4231.025879   \n",
       " 3           0         0  4186.538574  4168.717773  4266.794922  4229.230957   \n",
       " 4           0         0  4232.436035  4216.922852  4306.922852  4270.769043   \n",
       " \n",
       "         EEG.T7       EEG.P7       EEG.O1       EEG.O2  ...  POW.F8.Alpha  \\\n",
       " 0  4207.948730  4165.000000  4135.897461  4170.000000  ...      1.583895   \n",
       " 1  4209.615234  4152.436035  4130.128418  4149.487305  ...      1.709560   \n",
       " 2  4207.820313  4172.436035  4131.538574  4147.948730  ...      1.873591   \n",
       " 3  4202.179688  4155.384766  4128.333496  4151.666504  ...      2.110017   \n",
       " 4  4217.436035  4166.538574  4155.897461  4162.820313  ...      2.462552   \n",
       " \n",
       "    POW.F8.BetaL  POW.F8.BetaH  POW.F8.Gamma  POW.AF4.Theta  POW.AF4.Alpha  \\\n",
       " 0      0.504567      0.471979      0.138717       1.801014       1.504794   \n",
       " 1      0.606587      0.527616      0.155580       1.859177       1.379617   \n",
       " 2      0.795834      0.565414      0.170816       2.027946       1.283876   \n",
       " 3      1.021118      0.579656      0.180056       2.265952       1.306188   \n",
       " 4      1.230984      0.573620      0.181081       2.461205       1.522420   \n",
       " \n",
       "    POW.AF4.BetaL  POW.AF4.BetaH  POW.AF4.Gamma  subject_understood  \n",
       " 0       0.258570       0.435745       0.469483                   0  \n",
       " 1       0.317579       0.468416       0.642560                   0  \n",
       " 2       0.441925       0.494701       0.798197                   0  \n",
       " 3       0.616881       0.506062       0.886495                   0  \n",
       " 4       0.822598       0.498361       0.874455                   0  \n",
       " \n",
       " [5 rows x 87 columns],\n",
       " None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset to inspect its structure\n",
    "EEG.head(), EEG.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hVnYlZ2ZPnE_"
   },
   "outputs": [],
   "source": [
    "x= EEG.drop('subject_id', axis = 1)\n",
    "y = EEG['subject_id']\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HeC53hKAQEYn"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.9.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading tensorflow-2.9.0-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-5.28.1-cp39-cp39-win_amd64.whl (431 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (1.16.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.66.1-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (3.10.0.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (21.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (1.12.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (3.2.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.37.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.26.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.6.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow==2.9.0) (3.0.4)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Package(s) not found: tensorflow\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JRXdBZwzMVQv"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.platform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20616/2489813676.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmixed_precision\u001b[0m  \u001b[1;31m# Import directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mself_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.platform'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import mixed_precision  # Import directly\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8HdEg06JKQa"
   },
   "outputs": [],
   "source": [
    "# Enable mixed precision for energy-efficient training\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy) # Use set_global_policy instead of set_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8opJHqDkfM4d",
    "outputId": "35bacab8-6d10-41e0-95fc-7a38d3323353"
   },
   "outputs": [],
   "source": [
    "# Enable mixed precision for energy-efficient training\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_policy(policy)\n",
    "\n",
    "# Define the 14-layer Deep Contractive Autoencoder\n",
    "def build_energy_efficient_cae(input_dim, encoding_dim):\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder\n",
    "    encoded = layers.Dense(512, activation='swish')(input_layer)\n",
    "    encoded = layers.Dense(256, activation='swish')(encoded)\n",
    "    encoded = layers.Dense(128, activation='swish')(encoded)\n",
    "    encoded = layers.Dense(64, activation='swish')(encoded)\n",
    "    encoded = layers.Dense(32, activation='swish')(encoded)\n",
    "    encoded = layers.Dense(encoding_dim, activation='swish', name='encoded_layer')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = layers.Dense(32, activation='swish')(encoded)\n",
    "    decoded = layers.Dense(64, activation='swish')(decoded)\n",
    "    decoded = layers.Dense(128, activation='swish')(decoded)\n",
    "    decoded = layers.Dense(256, activation='swish')(decoded)\n",
    "    decoded = layers.Dense(512, activation='swish')(decoded)\n",
    "    output_layer = layers.Dense(input_dim, activation='elu')(decoded)\n",
    "\n",
    "    # Autoencoder Model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Assuming X_train is already defined and preprocessed\n",
    "#input_dim = x_train.shape[1]  # Dynamically set the input dimension based on X_train\n",
    "input_dim = 86 # Set this to the correct input dimension\n",
    "encoding_dim = 32  # Set the encoding dimension\n",
    "\n",
    "# Build the model with the dynamic input dimension\n",
    "autoencoder = build_energy_efficient_cae(input_dim, encoding_dim)\n",
    "\n",
    "# Learning rate scheduler for energy-efficient training\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    elif epoch < 200:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Compile the model with mixed precision and an energy-efficient optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Verify the shape of x_train\n",
    "#print(x_train.shape)\n",
    "\n",
    "# If the shape is not (None, 128), preprocess x_train to match the expected input dimension\n",
    "# For example, if x_train currently has 86 features, you might need to pad or transform it\n",
    "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#x_train = scaler.fit_transform(x_train)\n",
    "#x_test = scaler.transform(x_test)\n",
    "\n",
    "# Train the deeper model\n",
    "start_time = time.time()\n",
    "# Retrain the model with the correctly shaped x_train data\n",
    "history = autoencoder.fit(x_test, x_test,\n",
    "                          epochs=300,\n",
    "                          batch_size=32,\n",
    "                          validation_data=(x_test, x_test),\n",
    "                          shuffle=True,\n",
    "                          callbacks=[lr_schedule])\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time} seconds\")\n",
    "# Save the trained model\n",
    "autoencoder.save('energy_efficient_cae_14_layers.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation = autoencoder.evaluate(x_test, x_test)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "BNb39vv8tidy",
    "outputId": "0f55018e-f141-46c2-a79a-d895e011d8a6"
   },
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "#y_pred = autoencoder.predict(x_test) # Use the trained autoencoder to predict\n",
    "mse = mean_squared_error(x_test, y_pred) # Calculate MSE\n",
    "\n",
    "# Resource usage\n",
    "cpu_start = psutil.cpu_percent(interval=1)\n",
    "memory_start = psutil.virtual_memory().percent\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "cpu_end = psutil.cpu_percent(interval=1)\n",
    "memory_end = psutil.virtual_memory().percent\n",
    "\n",
    "energy_consumption = inference_time * (cpu_end - cpu_start + memory_end - memory_start)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Estimated Energy Consumption: {energy_consumption:.4f} (proxy value)\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Metrics Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['MSE'], [mse], color='skyblue')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "QVKFobV-bGWA",
    "outputId": "0423cc91-af6c-4b96-a280-d04dd234f835"
   },
   "outputs": [],
   "source": [
    "# You can also plot the encoded representations and the reconstructed outputs for further analysis\n",
    "#encoded_imgs = encoder.predict(x_test)\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Extract the encoder model\n",
    "encoder = models.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoded_layer').output)\n",
    "\n",
    "# You can also plot the encoded representations and the reconstructed outputs for further analysis\n",
    "#encoded_imgs = encoder.predict(x_test)  # Now using the defined 'encoder'\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Visualizing the original, encoded, and reconstructed signals\n",
    "n = 10  # Number of samples to plot\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.plot(x_test[i])\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    # Encoded\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.plot(encoded_imgs[i])\n",
    "    plt.title(\"Encoded\")\n",
    "\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.plot(decoded_imgs[i])\n",
    "    plt.title(\"Reconstructed\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "UwAQsq5Mq4yH",
    "outputId": "8d70c1b0-bd0e-4d6d-9ae3-56a697280c68"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy') # Changed from 'mean_squared_error' to 'accuracy'\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy') # Changed from 'val_mae' to 'val_accuracy'\n",
    "plt.title('Accuracy') # Changed title to reflect the correct metric\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional checks\n",
    "# Removed the accuracy calculation and overfitting check as they were not appropriate for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "7GszoVf2YPo1",
    "outputId": "527b261d-9c25-40c7-91f5-59810b15ca3b"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics # Import the metrics module from sklearn\n",
    "#predictions = autoencoder.predict(x_test)\n",
    "mse = metrics.mean_squared_error(x_test, predictions) # Use metrics.mean_squared_error\n",
    "r2 = metrics.r2_score(x_test, predictions) # Use metrics.r2_score\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "\n",
    "# Plotting Loss Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.legend()\n",
    "\n",
    "# Plotting Actual vs Predicted\n",
    "#plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_test[0], label='Actual Signal')\n",
    "plt.plot(predictions[0], label='Reconstructed Signal')\n",
    "plt.title('Actual vs Reconstructed EEG Signal')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "id": "8Ykm8FJhYnfC",
    "outputId": "a0a6d385-0517-405c-a94e-1e1b22e0af48"
   },
   "outputs": [],
   "source": [
    "# Function to plot original and reconstructed signals\n",
    "def plot_signal_comparison(original, reconstructed, num_signals=5):\n",
    "    plt.figure(figsize=(15, num_signals * 3))\n",
    "    for i in range(num_signals):\n",
    "        # Original Signal\n",
    "        plt.subplot(num_signals, 2, 2*i + 1)\n",
    "        plt.plot(original[i], color='blue')\n",
    "        plt.title(f'Original Signal {i+1}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Reconstructed Signal\n",
    "        plt.subplot(num_signals, 2, 2*i + 2)\n",
    "        plt.plot(reconstructed[i], color='red')\n",
    "        plt.title(f'Reconstructed Signal {i+1}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting comparison\n",
    "plot_signal_comparison(x_test, predictions, num_signals=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vwYaDk2TpPm"
   },
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "x_pred_train = autoencoder.predict(x_train)\n",
    "x_pred_test = autoencoder.predict(x_test)\n",
    "\n",
    "# Calculate accuracy for reconstruction (simplified)\n",
    "from sklearn.metrics import accuracy_score\n",
    "def calculate_accuracy(x, x_pred):\n",
    "    return np.mean([accuracy_score(np.round(x[i]), np.round(x_pred[i])) for i in range(len(x))])\n",
    "\n",
    "train_accuracy = calculate_accuracy(x_train, x_pred_train)\n",
    "test_accuracy = calculate_accuracy(x_test, x_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-gqu50-F0dh"
   },
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "x_pred_train = autoencoder.predict(x_train)\n",
    "x_pred_test = autoencoder.predict(x_test)\n",
    "\n",
    "# Calculate accuracy for reconstruction (simplified)\n",
    "from sklearn.metrics import accuracy_score # Import accuracy_score from sklearn.metrics\n",
    "\n",
    "def calculate_accuracy(x, x_pred):\n",
    "    return np.mean([accuracy_score(np.round(x[i]), np.round(x_pred[i])) for i in range(len(x))])\n",
    "\n",
    "train_accuracy = calculate_accuracy(x_train, x_pred_train)\n",
    "test_accuracy = calculate_accuracy(x_test, x_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zp1W4wcGa9k"
   },
   "outputs": [],
   "source": [
    "# Example performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Obtain predictions from the autoencoder\n",
    "predictions = autoencoder.predict(x_test)\n",
    "\n",
    "# Ensure that y_test and predictions have the same number of columns\n",
    "# You may need to adjust the reshaping based on the actual shape of your data\n",
    "if y_test.shape[1] != predictions.shape[1]:\n",
    "    y_test = y_test.reshape(predictions.shape)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn5rasLTGD61"
   },
   "outputs": [],
   "source": [
    "# Example performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Obtain predictions from the autoencoder\n",
    "predictions = autoencoder.predict(x_test)\n",
    "\n",
    "# Ensure that y_test and predictions have compatible shapes\n",
    "# Check if y_test needs to be reshaped or if a different target variable is needed\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqjrBzc6sna8"
   },
   "outputs": [],
   "source": [
    "# Example performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Obtain predictions from the autoencoder\n",
    "predictions = autoencoder.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcAHGrxgTjn1"
   },
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRMIKmlwGE9R"
   },
   "outputs": [],
   "source": [
    "# Example performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Obtain predictions from the autoencoder\n",
    "predictions = autoencoder.predict(x_test)\n",
    "\n",
    "# Ensure that y_test and predictions have compatible shapes\n",
    "# Check if y_test needs to be reshaped or if a different target variable is needed\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zx8HePY2GGBh"
   },
   "outputs": [],
   "source": [
    "# Example performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Obtain predictions from the autoencoder\n",
    "predictions = autoencoder.predict(x_test)\n",
    "\n",
    "# Ensure that y_test and predictions have compatible shapes\n",
    "# Check if y_test needs to be reshaped or if a different target variable is needed\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV9QBiSpTgas"
   },
   "outputs": [],
   "source": [
    "# Ensure accuracy is at least 90%\n",
    "assert test_accuracy >= 0.9, \"Accuracy did not reach 90%.\"\n",
    "\n",
    "# Plot loss graph\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BR0qBwgY0op",
    "outputId": "936f0027-2921-41f1-ba5c-c16c62b60133"
   },
   "outputs": [],
   "source": [
    "# Extracting latent features from test data\n",
    "latent_features = encoder.predict(x_test)\n",
    "\n",
    "print(f\"Shape of latent features: {latent_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7XIe54wTXnC"
   },
   "outputs": [],
   "source": [
    "# Define the 14-layer Deep Contractive Autoencoder with dynamic activation function\n",
    "def build_energy_efficient_cae(input_dim, encoding_dim, activation_choice):\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder\n",
    "    encoded = layers.Dense(512, activation=activation_choice)(input_layer)\n",
    "    encoded = layers.Dense(256, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(128, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(64, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(32, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(encoding_dim, activation=activation_choice, name='encoded_layer')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = layers.Dense(32, activation=activation_choice)(encoded)\n",
    "    decoded = layers.Dense(64, activation=activation_choice)(decoded)\n",
    "    decoded = layers.Dense(128, activation=activation_choice)(decoded)\n",
    "    decoded = layers.Dense(256, activation=activation_choice)(decoded)\n",
    "    decoded = layers.Dense(512, activation=activation_choice)(decoded)\n",
    "    output_layer = layers.Dense(input_dim, activation='elu')(decoded)\n",
    "\n",
    "    # Autoencoder Model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjMSCk7KTvlt"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameter choice for activation (dynamic activation function)\n",
    "activation_choice = tf.keras.activations.swish  # Options: tf.keras.activations.relu, tf.keras.activations.elu, tf.keras.activations.swish\n",
    "\n",
    "# Input dimensions (replace with actual dimensions from your dataset)\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "# Build the model with the chosen activation function\n",
    "autoencoder = build_energy_efficient_cae(input_dim, encoding_dim, activation_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mki3d0H3TsAF"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler for energy-efficient training\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    elif epoch < 200:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "pUvsqyIXTc7I",
    "outputId": "a2765e82-339c-4cdf-9737-42bb81911a1b"
   },
   "outputs": [],
   "source": [
    "# Start timing the training process\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(x_train, x_train,  # Make sure x_train is preprocessed\n",
    "                          epochs=300,\n",
    "                          batch_size=32,\n",
    "                          validation_data=(x_test, x_test),  # Ensure x_test is preprocessed\n",
    "                          shuffle=True,\n",
    "                          callbacks=[lr_schedule])\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "autoencoder.save('energy_efficient_cae_14_layers.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation = autoencoder.evaluate(x_test, x_test)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "# Print CPU usage and memory usage for energy-efficient tracking\n",
    "cpu_usage = psutil.cpu_percent()\n",
    "memory_usage = psutil.virtual_memory().percent\n",
    "print(f\"CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRI5CGbbUYYx",
    "outputId": "57f6c51a-392f-4c28-ae9b-8ab8ca00de1b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Define a function to build the 14-layer Deep Contractive Autoencoder with dynamic variables\n",
    "def build_energy_efficient_cae(input_dim, encoding_dim, activation_choice):\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder with dynamic activation function\n",
    "    encoded = layers.Dense(512, activation=activation_choice)(input_layer)\n",
    "    encoded = layers.Dense(256, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(128, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(64, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(32, activation=activation_choice)(encoded)\n",
    "    encoded = layers.Dense(encoding_dim, activation=activation_choice, name='encoded_layer')(encoded)\n",
    "\n",
    "    # Decoder with dynamic activation function\n",
    "    decoded = layers.Dense(32, activation=activation_choice)(encoded)\n",
    "    decoded = layers.Dense(64, activation=activation_choice)(decoded)\n",
    "    decoded = layers.Dense(128, activation=activation_choice)(decoded)\n",
    "    decoded = layers.Dense(256, activation=activation_choice)(decoded)\n",
    "    decoded = layers.Dense(512, activation=activation_choice)(decoded)\n",
    "    output_layer = layers.Dense(input_dim, activation='elu')(decoded)  # ELU for the output layer\n",
    "\n",
    "    # Autoencoder Model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Define hyperparameters as dynamic variables\n",
    "#input_dim = 86  # Replace with actual input dimension\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 32  # Latent encoding dimension, adjust as needed\n",
    "\n",
    "# Dynamic selection for activation function (Swish, ReLU, ELU)\n",
    "activation_choice = tf.keras.activations.swish  # You can also use 'relu' or 'elu'\n",
    "\n",
    "# Dynamic learning rate for Adam optimizer\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.01  # Dynamic weight decay for regularization\n",
    "\n",
    "\n",
    "# Build the model with dynamic input dimension, encoding dimension, and activation\n",
    "autoencoder = build_energy_efficient_cae(input_dim, encoding_dim, activation_choice)\n",
    "\n",
    "# Define dynamic learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    elif epoch < 200:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Build the model with dynamic input dimension, encoding dimension, and activation\n",
    "autoencoder = build_energy_efficient_cae(input_dim, encoding_dim, activation_choice)\n",
    "\n",
    "# Dynamic optimizer selection (Adam, RMSprop, SGD, etc.)\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Dynamic learning rate\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Compile the model with dynamic optimizer and loss function\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Simulate data for training and testing (replace these with your actual dataset)\n",
    "#x_train = tf.random.normal((1000, input_dim))  # Replace with your actual preprocessed dataset\n",
    "#x_test = tf.random.normal((200, input_dim))    # Replace with your actual preprocessed test dataset\n",
    "\n",
    "# Start timing the training process\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model with dynamic batch size, epochs, and data\n",
    "history = autoencoder.fit(x_train, x_train,  # Training data\n",
    "                          epochs=300,  # Dynamic number of epochs\n",
    "                          batch_size=32,  # Dynamic batch size\n",
    "                          validation_data=(x_test, x_test),  # Validation data\n",
    "                          shuffle=True,\n",
    "                          callbacks=[lr_schedule])\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "autoencoder.save('energy_efficient_cae_14_layers_dynamic.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation = autoencoder.evaluate(x_test, x_test)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n",
    "\n",
    "# Print CPU usage and memory usage for energy-efficient tracking\n",
    "cpu_usage = psutil.cpu_percent()\n",
    "memory_usage = psutil.virtual_memory().percent\n",
    "print(f\"CPU Usage: {cpu_usage}%, Memory Usage: {memory_usage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "aEGhF4nZvrbE",
    "outputId": "1447d9b1-abc4-425b-8f9c-74c5aba97a37"
   },
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "#y_pred = autoencoder.predict(x_test) # Use the trained autoencoder to predict\n",
    "mse = mean_squared_error(x_test, y_pred) # Calculate MSE\n",
    "\n",
    "# Resource usage\n",
    "cpu_start = psutil.cpu_percent(interval=1)\n",
    "memory_start = psutil.virtual_memory().percent\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "cpu_end = psutil.cpu_percent(interval=1)\n",
    "memory_end = psutil.virtual_memory().percent\n",
    "\n",
    "energy_consumption = inference_time * (cpu_end - cpu_start + memory_end - memory_start)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "print(f\"Estimated Energy Consumption: {energy_consumption:.4f} (proxy value)\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Metrics Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['MSE'], [mse], color='skyblue')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "fEsUct_AvrbE",
    "outputId": "8f0206b9-6afa-4f4b-e7eb-7636027ff6a6"
   },
   "outputs": [],
   "source": [
    "# You can also plot the encoded representations and the reconstructed outputs for further analysis\n",
    "#encoded_imgs = encoder.predict(x_test)\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Extract the encoder model\n",
    "encoder = models.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoded_layer').output)\n",
    "\n",
    "# You can also plot the encoded representations and the reconstructed outputs for further analysis\n",
    "#encoded_imgs = encoder.predict(x_test)  # Now using the defined 'encoder'\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Visualizing the original, encoded, and reconstructed signals\n",
    "n = 10  # Number of samples to plot\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.plot(x_test[i])\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    # Encoded\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.plot(encoded_imgs[i])\n",
    "    plt.title(\"Encoded\")\n",
    "\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.plot(decoded_imgs[i])\n",
    "    plt.title(\"Reconstructed\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DJ8_r7PvrbF"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy') # Changed from 'mean_squared_error' to 'accuracy'\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy') # Changed from 'val_mae' to 'val_accuracy'\n",
    "plt.title('Accuracy') # Changed title to reflect the correct metric\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional checks\n",
    "# Removed the accuracy calculation and overfitting check as they were not appropriate for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IRkuUcZvrbF"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics # Import the metrics module from sklearn\n",
    "#predictions = autoencoder.predict(x_test)\n",
    "mse = metrics.mean_squared_error(x_test, predictions) # Use metrics.mean_squared_error\n",
    "r2 = metrics.r2_score(x_test, predictions) # Use metrics.r2_score\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "\n",
    "# Plotting Loss Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.legend()\n",
    "\n",
    "# Plotting Actual vs Predicted\n",
    "#plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_test[0], label='Actual Signal')\n",
    "plt.plot(predictions[0], label='Reconstructed Signal')\n",
    "plt.title('Actual vs Reconstructed EEG Signal')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqVyFlwsvrbF"
   },
   "outputs": [],
   "source": [
    "# Function to plot original and reconstructed signals\n",
    "def plot_signal_comparison(original, reconstructed, num_signals=5):\n",
    "    plt.figure(figsize=(15, num_signals * 3))\n",
    "    for i in range(num_signals):\n",
    "        # Original Signal\n",
    "        plt.subplot(num_signals, 2, 2*i + 1)\n",
    "        plt.plot(original[i], color='blue')\n",
    "        plt.title(f'Original Signal {i+1}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Reconstructed Signal\n",
    "        plt.subplot(num_signals, 2, 2*i + 2)\n",
    "        plt.plot(reconstructed[i], color='red')\n",
    "        plt.title(f'Reconstructed Signal {i+1}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting comparison\n",
    "plot_signal_comparison(x_test, predictions, num_signals=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uudWHMkfvIfc"
   },
   "outputs": [],
   "source": [
    "activation = hp.Choice('activation', ['relu', 'elu', 'swish'])\n",
    "    if activation == 'swish':\n",
    "        activation_func = swish\n",
    "    elif activation == 'elu':\n",
    "        activation_func = 'elu'\n",
    "    else:\n",
    "        activation_func = 'relu'\n",
    "\n",
    "optimizer = hp.Choice('optimizer', ['AdamW', 'RMSprop', 'SGD'])\n",
    "    if activation == 'AdamW':\n",
    "        # Dynamic optimizer selection (AdamW with weight decay)\n",
    "        tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    elif activation == 'RMSprop':\n",
    "         tf.keras.optimizers.RMSprop(learning_rate=learning_rate)  # Dynamic learning rate\n",
    "    else:\n",
    "       tf.keras.optimizers.SGD(learning_rate=learning_rate)  # Dynamic learning rate\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
